---
title: Métriques ML.NET
description: Découvrir les métriques qui sont utilisées pour évaluer les performances d’un modèle ML.NET
ms.date: 04/29/2019
author: natke
ms.openlocfilehash: 362f2f382d050ff9ae246af2dffe3e15d22452eb
ms.sourcegitcommit: 944ddc52b7f2632f30c668815f92b378efd38eea
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 11/03/2019
ms.locfileid: "73460729"
---
# <a name="model-evaluation-metrics-in-mlnet"></a><span data-ttu-id="95194-103">Métriques d’évaluation de modèle dans ML.NET</span><span class="sxs-lookup"><span data-stu-id="95194-103">Model evaluation metrics in ML.NET</span></span>

## <a name="metrics-for-binary-classification"></a><span data-ttu-id="95194-104">Métriques de classification binaire</span><span class="sxs-lookup"><span data-stu-id="95194-104">Metrics for Binary Classification</span></span>

| <span data-ttu-id="95194-105">Métriques</span><span class="sxs-lookup"><span data-stu-id="95194-105">Metrics</span></span>   |      <span data-ttu-id="95194-106">Description</span><span class="sxs-lookup"><span data-stu-id="95194-106">Description</span></span>      |  <span data-ttu-id="95194-107">Recherche</span><span class="sxs-lookup"><span data-stu-id="95194-107">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="95194-108">**Précision**</span><span class="sxs-lookup"><span data-stu-id="95194-108">**Accuracy**</span></span> |  <span data-ttu-id="95194-109">La [précision](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) est la proportion de prédictions correctes avec un jeu de données de test.</span><span class="sxs-lookup"><span data-stu-id="95194-109">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) is the proportion of correct predictions with a test data set.</span></span> <span data-ttu-id="95194-110">Elle représente le rapport entre le nombre de prédictions correctes et le nombre total d’échantillons d’entrée.</span><span class="sxs-lookup"><span data-stu-id="95194-110">It is the ratio of number of correct predictions to the total number of input samples.</span></span> <span data-ttu-id="95194-111">Elle n’est efficace que s’il existe un nombre similaire d’échantillons dans chaque classe.</span><span class="sxs-lookup"><span data-stu-id="95194-111">It works well only if there are similar number of samples belonging to each class.</span></span>| <span data-ttu-id="95194-112">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-112">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="95194-113">Toutefois, la valeur exacte 1,00 indique un problème (en règle générale, une fuite d’étiquette/cible, un surapprentissage ou un test avec des données d’entraînement).</span><span class="sxs-lookup"><span data-stu-id="95194-113">But exactly 1.00 indicates an issue (commonly: label/target leakage, over-fitting, or testing with training data).</span></span> <span data-ttu-id="95194-114">Quand les données de test sont asymétriques (la plupart des instances appartiennent à une des classes), que le jeu de données est très petit ou que les scores approchent 0,00 ou 1,00, la précision ne capture pas vraiment l’efficacité d’un classifieur, ce qui vous oblige à vérifier des métriques supplémentaires.</span><span class="sxs-lookup"><span data-stu-id="95194-114">When the test data is unbalanced (where most of the instances belong to one of the classes), the dataset is very small, or scores approach 0.00 or 1.00, then accuracy doesn’t really capture the effectiveness of a classifier and you need to check additional metrics.</span></span> |
| <span data-ttu-id="95194-115">**AUC**</span><span class="sxs-lookup"><span data-stu-id="95194-115">**AUC**</span></span> |    <span data-ttu-id="95194-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) ou *zone sous la courbe*: cela mesure la zone sous la courbe créée en balayant le taux réel positif par rapport au taux de faux positifs.</span><span class="sxs-lookup"><span data-stu-id="95194-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or *Area under the curve*: This is measuring the area under the curve created by sweeping the true positive rate vs. the false positive rate.</span></span>  |   <span data-ttu-id="95194-117">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-117">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="95194-118">La valeur doit être supérieure à 0,50 pour qu’un modèle soit acceptable ; un modèle avec une métrique AUC inférieure ou égale à 0,50 est sans intérêt.</span><span class="sxs-lookup"><span data-stu-id="95194-118">It should be greater than 0.50 for a model to be acceptable; a model with AUC of 0.50 or less is worthless.</span></span> |
| <span data-ttu-id="95194-119">**Zone sous une courbe de précision/rappel**</span><span class="sxs-lookup"><span data-stu-id="95194-119">**AUCPR**</span></span> | <span data-ttu-id="95194-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) ou *zone sous la courbe d’une courbe de rappel de précision*: mesure utile du succès de la prédiction lorsque les classes sont très déséquilibrées (jeux de données très faussés).</span><span class="sxs-lookup"><span data-stu-id="95194-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) or *Area under the curve of a Precision-Recall curve*: Useful measure of success of prediction when the classes are very imbalanced (highly skewed datasets).</span></span> |  <span data-ttu-id="95194-121">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-121">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="95194-122">Des scores élevés proches de 1,00 montrent que le classifieur retourne des résultats précis (précision élevée) ainsi que la majorité de tous les résultats positifs (rappel élevé).</span><span class="sxs-lookup"><span data-stu-id="95194-122">High scores close to 1.00 show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</span></span> |
| <span data-ttu-id="95194-123">**Score F1**</span><span class="sxs-lookup"><span data-stu-id="95194-123">**F1-score**</span></span> | <span data-ttu-id="95194-124">[Score F1](https://en.wikipedia.org/wiki/F1_score) également appelé *balanced F-score or F-measure*.</span><span class="sxs-lookup"><span data-stu-id="95194-124">[F1 score](https://en.wikipedia.org/wiki/F1_score) also known as *balanced F-score or F-measure*.</span></span> <span data-ttu-id="95194-125">Il s’agit de la moyenne harmonique de la précision et du rappel.</span><span class="sxs-lookup"><span data-stu-id="95194-125">It's the harmonic mean of the precision and recall.</span></span> <span data-ttu-id="95194-126">Le score F1 est utile quand vous souhaitez rechercher un équilibre entre la précision et le rappel.</span><span class="sxs-lookup"><span data-stu-id="95194-126">F1 Score is helpful when you want to seek a balance between Precision and Recall.</span></span>| <span data-ttu-id="95194-127">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-127">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="95194-128">Un score F1 atteint sa meilleure valeur à 1,00 et la pire à 0,00.</span><span class="sxs-lookup"><span data-stu-id="95194-128">An F1 score reaches its best value at 1.00 and worst score at 0.00.</span></span> <span data-ttu-id="95194-129">Il vous indique le degré de précision de votre classifieur.</span><span class="sxs-lookup"><span data-stu-id="95194-129">It tells you how precise your classifier is.</span></span> |

<span data-ttu-id="95194-130">Pour plus d’informations sur les métriques de classification binaire, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="95194-130">For further details on binary classification metrics read the following articles:</span></span>

- [<span data-ttu-id="95194-131">Accuracy, Precision, Recall or F1?</span><span class="sxs-lookup"><span data-stu-id="95194-131">Accuracy, Precision, Recall or F1?</span></span>](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
- [<span data-ttu-id="95194-132">Classe BinaryClassificationMetrics</span><span class="sxs-lookup"><span data-stu-id="95194-132">Binary Classification Metrics class</span></span>](xref:Microsoft.ML.Data.BinaryClassificationMetrics)
- [<span data-ttu-id="95194-133">The Relationship Between Precision-Recall and ROC Curves</span><span class="sxs-lookup"><span data-stu-id="95194-133">The Relationship Between Precision-Recall and ROC Curves</span></span>](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)

## <a name="metrics-for-multi-class-classification"></a><span data-ttu-id="95194-134">Métriques de classification multiclasse</span><span class="sxs-lookup"><span data-stu-id="95194-134">Metrics for Multi-class Classification</span></span>

| <span data-ttu-id="95194-135">Métriques</span><span class="sxs-lookup"><span data-stu-id="95194-135">Metrics</span></span>   |      <span data-ttu-id="95194-136">Description</span><span class="sxs-lookup"><span data-stu-id="95194-136">Description</span></span>      |  <span data-ttu-id="95194-137">Recherche</span><span class="sxs-lookup"><span data-stu-id="95194-137">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="95194-138">**Micro-précision**</span><span class="sxs-lookup"><span data-stu-id="95194-138">**Micro-Accuracy**</span></span> |  <span data-ttu-id="95194-139">La [précision micro-moyenne](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) agrège les contributions de toutes les classes pour calculer la métrique moyenne.</span><span class="sxs-lookup"><span data-stu-id="95194-139">[Micro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) aggregates the contributions of all classes to compute the average metric.</span></span> <span data-ttu-id="95194-140">Il s’agit de la fraction d’instances correctement prédites.</span><span class="sxs-lookup"><span data-stu-id="95194-140">It is the fraction of instances predicted correctly.</span></span> <span data-ttu-id="95194-141">La micro-moyenne ne tient pas compte de l’appartenance aux classes.</span><span class="sxs-lookup"><span data-stu-id="95194-141">The micro-average does not take class membership into account.</span></span> <span data-ttu-id="95194-142">Fondamentalement, chaque paire exemple-classe contribue de manière égale à la métrique de précision.</span><span class="sxs-lookup"><span data-stu-id="95194-142">Basically, every sample-class pair contributes equally to the accuracy metric.</span></span> | <span data-ttu-id="95194-143">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-143">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="95194-144">Dans une tâche de classification multiclasse, la micro-précision est préférable à la macro-précision si vous suspectez un déséquilibre de classes éventuel (</span><span class="sxs-lookup"><span data-stu-id="95194-144">In a multi-class classification task, micro-accuracy is preferable over macro-accuracy if you suspect there might be class imbalance (i.e</span></span> <span data-ttu-id="95194-145">vous avez peut-être beaucoup plus d’exemples d’une classe que d’autres classes).</span><span class="sxs-lookup"><span data-stu-id="95194-145">you may have many more examples of one class than of other classes).</span></span>|
| <span data-ttu-id="95194-146">**Macro-précision**</span><span class="sxs-lookup"><span data-stu-id="95194-146">**Macro-Accuracy**</span></span> | <span data-ttu-id="95194-147">La [précision macro-moyenne](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) est la précision moyenne au niveau de la classe.</span><span class="sxs-lookup"><span data-stu-id="95194-147">[Macro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) is the average accuracy at the class level.</span></span> <span data-ttu-id="95194-148">La précision pour chaque classe est calculée et la macro-précision est la moyenne de ces précisions.</span><span class="sxs-lookup"><span data-stu-id="95194-148">The accuracy for each class is computed and the macro-accuracy is the average of these accuracies.</span></span> <span data-ttu-id="95194-149">Fondamentalement, chaque classe contribue de manière égale à la métrique de précision.</span><span class="sxs-lookup"><span data-stu-id="95194-149">Basically, every class contributes equally to the accuracy metric.</span></span> <span data-ttu-id="95194-150">Les classes minoritaires sont aussi importantes que les classes plus grandes.</span><span class="sxs-lookup"><span data-stu-id="95194-150">Minority classes are given equal weight as the larger classes.</span></span> <span data-ttu-id="95194-151">La métrique de macro-moyenne donne la même pondération à chaque classe, quel que soit le nombre d’instances de cette classe contenues dans le jeu de données.</span><span class="sxs-lookup"><span data-stu-id="95194-151">The macro-average metric gives the same weight to each class, no matter how many instances from that class the dataset contains.</span></span> |  <span data-ttu-id="95194-152">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-152">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="95194-153">La métrique est calculée de manière indépendante pour chaque classe, puis la moyenne est calculée (toutes les classes étant ainsi traitées de façon égale)</span><span class="sxs-lookup"><span data-stu-id="95194-153">It computes the metric independently for each class and then takes the average (hence treating all classes equally)</span></span> |
| <span data-ttu-id="95194-154">**Perte logarithmique**</span><span class="sxs-lookup"><span data-stu-id="95194-154">**Log-loss**</span></span>| <span data-ttu-id="95194-155">La [perte logarithmique](http://wiki.fast.ai/index.php/Log_Loss) mesure les performances d’un modèle de classification où l’entrée de prédiction est une valeur de probabilité comprise entre 0,00 et 1,00.</span><span class="sxs-lookup"><span data-stu-id="95194-155">[Logarithmic loss](http://wiki.fast.ai/index.php/Log_Loss) measures the performance of a classification model where the prediction input is a probability value between 0.00 and 1.00.</span></span> <span data-ttu-id="95194-156">La perte logarithmique augmente à mesure que la probabilité prédite diffère de l’étiquette réelle.</span><span class="sxs-lookup"><span data-stu-id="95194-156">Log-loss increases as the predicted probability diverges from the actual label.</span></span> | <span data-ttu-id="95194-157">**Plus la précision est proche de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-157">**The closer to 0.00, the better**.</span></span> <span data-ttu-id="95194-158">Un modèle parfait aurait une perte logarithmique de 0,00.</span><span class="sxs-lookup"><span data-stu-id="95194-158">A perfect model would have a log-loss of 0.00.</span></span> <span data-ttu-id="95194-159">L’objectif de nos modèles Machine Learning consiste à réduire cette valeur.</span><span class="sxs-lookup"><span data-stu-id="95194-159">The goal of our machine learning models is to minimize this value.</span></span>|
| <span data-ttu-id="95194-160">**Réduction de la perte logarithmique**</span><span class="sxs-lookup"><span data-stu-id="95194-160">**Log-Loss Reduction**</span></span> | <span data-ttu-id="95194-161">La [réduction de la perte logarithmique](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) peut être interprétée comme exprimant l’avantage du classifieur par rapport à une prédiction aléatoire.</span><span class="sxs-lookup"><span data-stu-id="95194-161">[Logarithmic loss reduction](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) can be interpreted as the advantage of the classifier over a random prediction.</span></span>| <span data-ttu-id="95194-162">**Elle est comprise entre -inf et 1,00, où 1,00 correspond à des prédictions parfaites et 0,00 à des prédictions moyennes**.</span><span class="sxs-lookup"><span data-stu-id="95194-162">**Ranges from -inf and 1.00, where 1.00 is perfect predictions and 0.00 indicates mean predictions**.</span></span> <span data-ttu-id="95194-163">Par exemple, si la valeur est égale à 0,20, elle peut être interprétée comme « la probabilité d’une prédiction correcte est 20 % meilleure qu’une estimation aléatoire ».</span><span class="sxs-lookup"><span data-stu-id="95194-163">For example, if the value equals 0.20, it can be interpreted as "the probability of a correct prediction is 20% better than random guessing"</span></span>|

<span data-ttu-id="95194-164">La micro-précision est généralement mieux alignée sur les besoins métier de prédictions de ML.</span><span class="sxs-lookup"><span data-stu-id="95194-164">Micro-accuracy is generally better aligned with the business needs of ML predictions.</span></span> <span data-ttu-id="95194-165">Si vous souhaitez sélectionner une seule métrique pour choisir la qualité d’une tâche de classification multiclasse, ce doit généralement être la micro-précision.</span><span class="sxs-lookup"><span data-stu-id="95194-165">If you want to select a single metric for choosing the quality of a multiclass classification task, it should usually be micro-accuracy.</span></span>

<span data-ttu-id="95194-166">Prenons l’exemple d’une tâche de classification de ticket de support (mappage des tickets entrants aux équipes de support technique) :</span><span class="sxs-lookup"><span data-stu-id="95194-166">Example, for a support ticket classification task: (maps incoming tickets to support teams)</span></span>

- <span data-ttu-id="95194-167">Micro-précision : avec quelle fréquence un ticket entrant est-il orienté vers l’équipe appropriée ?</span><span class="sxs-lookup"><span data-stu-id="95194-167">Micro-accuracy -- how often does an incoming ticket get classified to the right team?</span></span>
- <span data-ttu-id="95194-168">Macro-précision : pour une équipe moyenne, avec quelle fréquence un ticket entrant est-il correct pour l’équipe concernée ?</span><span class="sxs-lookup"><span data-stu-id="95194-168">Macro-accuracy -- for an average team, how often is an incoming ticket correct for their team?</span></span>

<span data-ttu-id="95194-169">La macro-précision accorde une pondération supérieure aux petites équipes dans cet exemple ; une petite équipe qui n’obtient que 10 tickets par an compte autant qu’une grande équipe qui obtient 10 000 tickets par an.</span><span class="sxs-lookup"><span data-stu-id="95194-169">Macro-accuracy overweights small teams in this example; a small team which gets only 10 tickets per year counts as much as a large team with 10k tickets per year.</span></span> <span data-ttu-id="95194-170">Dans ce cas, la micro-précision présente une meilleure corrélation avec le besoin métier exprimé par « combien de temps et d’argent l’entreprise peut-elle économiser en automatisant mon processus de routage des tickets ».</span><span class="sxs-lookup"><span data-stu-id="95194-170">Micro-accuracy in this case correlates better with the business need of, "how much time/money can the company save by automating my ticket routing process".</span></span>

<span data-ttu-id="95194-171">Pour plus d’informations sur les métriques de classification multiclasse, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="95194-171">For further details on multi-class classification metrics read the following articles:</span></span>

- [<span data-ttu-id="95194-172">Micro- and Macro-average of Precision, Recall and F-Score</span><span class="sxs-lookup"><span data-stu-id="95194-172">Micro- and Macro-average of Precision, Recall and F-Score</span></span>](https://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html)
- [<span data-ttu-id="95194-173">Multiclass Classification with Imbalanced Dataset</span><span class="sxs-lookup"><span data-stu-id="95194-173">Multiclass Classification with Imbalanced Dataset</span></span>](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)

## <a name="metrics-for-regression"></a><span data-ttu-id="95194-174">Métriques de régression</span><span class="sxs-lookup"><span data-stu-id="95194-174">Metrics for Regression</span></span>

| <span data-ttu-id="95194-175">Métriques</span><span class="sxs-lookup"><span data-stu-id="95194-175">Metrics</span></span>   |      <span data-ttu-id="95194-176">Description</span><span class="sxs-lookup"><span data-stu-id="95194-176">Description</span></span>      |  <span data-ttu-id="95194-177">Recherche</span><span class="sxs-lookup"><span data-stu-id="95194-177">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="95194-178">**R carré**</span><span class="sxs-lookup"><span data-stu-id="95194-178">**R-Squared**</span></span> |  <span data-ttu-id="95194-179">Le *coefficient de détermination*, ou [R carré (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), représente la puissance prédictive du modèle sous la forme d’une valeur comprise entre -inf et 1,00.</span><span class="sxs-lookup"><span data-stu-id="95194-179">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), or *Coefficient of determination* represents the predictive power of the model as a value between -inf and 1.00.</span></span> <span data-ttu-id="95194-180">1,00 signifie un ajustement parfait ; l’ajustement peut être arbitrairement médiocre, les scores pouvant alors être négatifs.</span><span class="sxs-lookup"><span data-stu-id="95194-180">1.00 means there is a perfect fit, and the fit can be arbitrarily poor so the scores can be negative.</span></span> <span data-ttu-id="95194-181">Un score de 0,00 signifie que le modèle devine la valeur attendue pour l’étiquette.</span><span class="sxs-lookup"><span data-stu-id="95194-181">A score of 0.00 means the model is guessing the expected value for the label.</span></span> <span data-ttu-id="95194-182">R2 mesure la proximité des valeurs de données de test réelles des valeurs prédites.</span><span class="sxs-lookup"><span data-stu-id="95194-182">R2 measures how close the actual test data values are to the predicted values.</span></span> | <span data-ttu-id="95194-183">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-183">**The closer to 1.00, the better quality**.</span></span> <span data-ttu-id="95194-184">Cependant, de faibles valeurs de coefficient de détermination (par exemple 0,50) peuvent parfois être tout à fait normales ou suffisantes pour votre scénario, alors que des valeurs élevées ne conviennent pas toujours et peuvent être suspectes.</span><span class="sxs-lookup"><span data-stu-id="95194-184">However, sometimes low R-squared values (such as 0.50) can be entirely normal or good enough for your scenario and high R-squared values are not always good and be suspicious.</span></span> |
| <span data-ttu-id="95194-185">**Perte absolue**</span><span class="sxs-lookup"><span data-stu-id="95194-185">**Absolute-loss**</span></span> |  <span data-ttu-id="95194-186">La [perte absolue](https://en.wikipedia.org/wiki/Mean_absolute_error) ou *erreur d’absolue moyenne (MAE)* mesure la proximité des prédictions des résultats réels.</span><span class="sxs-lookup"><span data-stu-id="95194-186">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) or *Mean absolute error (MAE)* measures how close the predictions are to the actual outcomes.</span></span> <span data-ttu-id="95194-187">Il s’agit de la moyenne de toutes les erreurs du modèle, où l’erreur de modèle est la distance absolue entre la valeur d’étiquette prédite et la valeur d’étiquette correcte.</span><span class="sxs-lookup"><span data-stu-id="95194-187">It is the average of all the model errors, where model error is the absolute distance between the predicted label value and the correct label value.</span></span> <span data-ttu-id="95194-188">Cette erreur de prédiction est calculée pour chaque enregistrement du jeu de données de test.</span><span class="sxs-lookup"><span data-stu-id="95194-188">This prediction error is calculated for each record of the test data set.</span></span> <span data-ttu-id="95194-189">Enfin, la valeur moyenne est calculée pour toutes les erreurs d’absolue enregistrées.</span><span class="sxs-lookup"><span data-stu-id="95194-189">Finally, the mean value is calculated for all recorded absolute errors.</span></span>| <span data-ttu-id="95194-190">**Plus la précision est proche de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-190">**The closer to 0.00, the better quality.**</span></span> <span data-ttu-id="95194-191">Notez que l’erreur d’absolue moyenne utilise la même échelle que les données mesurées (elle n’est pas normalisée sur une plage spécifique).</span><span class="sxs-lookup"><span data-stu-id="95194-191">Note that the mean absolute error uses the same scale as the data being measured (is not normalized to specific range).</span></span> <span data-ttu-id="95194-192">Vous ne pouvez utiliser l’erreur absolue, l’erreur quadratique moyenne et la racine de l’erreur quadratique moyenne que pour comparer des modèles pour le même jeu de données ou pour un jeu de données présentant une distribution similaire des valeurs d’étiquette.</span><span class="sxs-lookup"><span data-stu-id="95194-192">Absolute-loss, Squared-loss, and RMS-loss can only be used to make comparisons between models for the same dataset or dataset with a similar label value distribution.</span></span> |
| <span data-ttu-id="95194-193">**Erreur quadratique**</span><span class="sxs-lookup"><span data-stu-id="95194-193">**Squared-loss**</span></span> |  <span data-ttu-id="95194-194">L’[Erreur quadratique](https://en.wikipedia.org/wiki/Mean_squared_error) ou *Erreur quadratique moyenne (MSE)* , également appelée *Écart quadratique moyen (MSD)* , indique la proximité d’une ligne de régression d’un ensemble de valeurs de données de test.</span><span class="sxs-lookup"><span data-stu-id="95194-194">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) or *Mean Squared Error (MSE)*, also called *Mean Squared Deviation (MSD)*, tells you how close a regression line is to a set of test data values.</span></span> <span data-ttu-id="95194-195">Elle élève au carré les distances entre les points et la ligne de régression (ces distances sont les erreurs E).</span><span class="sxs-lookup"><span data-stu-id="95194-195">It does this by taking the distances from the points to the regression line (these distances are the errors E) and squaring them.</span></span> <span data-ttu-id="95194-196">L’élévation au carré attribue une pondération supérieure aux différences plus grandes.</span><span class="sxs-lookup"><span data-stu-id="95194-196">The squaring gives more weight to larger differences.</span></span> | <span data-ttu-id="95194-197">Elle est toujours non négative, et **plus les valeurs sont proches de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-197">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="95194-198">En fonction de vos données, il peut s’avérer impossible d’obtenir une valeur très petite pour l’erreur quadratique moyenne.</span><span class="sxs-lookup"><span data-stu-id="95194-198">Depending on your data, it may be impossible to get a very small value for the mean squared error.</span></span>|
| <span data-ttu-id="95194-199">**Racine de l’erreur quadratique**</span><span class="sxs-lookup"><span data-stu-id="95194-199">**RMS-loss**</span></span> |  <span data-ttu-id="95194-200">La [racine de l’erreur quadratique](https://en.wikipedia.org/wiki/Root-mean-square_deviation) ou *racine de l’erreur quadratique moyenne*, également appelée *racine de l’écart quadratique moyen*, mesure la différence entre les valeurs prédites par un modèle et les valeurs réellement observées à partir de l’environnement en cours de modélisation.</span><span class="sxs-lookup"><span data-stu-id="95194-200">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or *Root Mean Squared Error (RMSE)* (also called *Root Mean Square Deviation, RMSD*), measures the difference between values predicted by a model and the values actually observed from the environment that is being modeled.</span></span> <span data-ttu-id="95194-201">La racine de l’erreur quadratique moyenne est la racine carrée de l’erreur quadratique moyenne et a les mêmes unités que l’étiquette, à l’image de l’erreur absolue, bien que les différences plus grandes se voient attribuer une pondération supérieure.</span><span class="sxs-lookup"><span data-stu-id="95194-201">RMS-loss is the square root of Squared-loss and has the same units as the label, similar to the absolute-loss though giving more weight to larger differences.</span></span> <span data-ttu-id="95194-202">La racine de l’erreur quadratique moyenne est couramment utilisée dans les domaines de la climatologie, des prévisions et de l’analyse de régression pour vérifier des résultats expérimentaux.</span><span class="sxs-lookup"><span data-stu-id="95194-202">Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</span></span> | <span data-ttu-id="95194-203">Elle est toujours non négative, et **plus les valeurs sont proches de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="95194-203">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="95194-204">La racine de l’erreur quadratique moyenne est une mesure de précision, qui compare les erreurs de prévision de différents modèles pour un jeu de données particulier et non entre plusieurs jeux de données, étant dépendante de l’échelle.</span><span class="sxs-lookup"><span data-stu-id="95194-204">RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.</span></span>|

<span data-ttu-id="95194-205">Pour plus d’informations sur les métriques de régression, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="95194-205">For further details on regression metrics, read the following articles:</span></span>

- [<span data-ttu-id="95194-206">Analyse de régression : comment interpréter la R-squareie et évaluer l’adéquation ?</span><span class="sxs-lookup"><span data-stu-id="95194-206">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</span></span>](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
- [<span data-ttu-id="95194-207">How To Interpret R-squared in Regression Analysis</span><span class="sxs-lookup"><span data-stu-id="95194-207">How To Interpret R-squared in Regression Analysis</span></span>](https://statisticsbyjim.com/regression/interpret-r-squared-regression)
- [<span data-ttu-id="95194-208">R-Squared Definition</span><span class="sxs-lookup"><span data-stu-id="95194-208">R-Squared Definition</span></span>](https://www.investopedia.com/terms/r/r-squared.asp)
- [<span data-ttu-id="95194-209">Mean Squared Error Definition</span><span class="sxs-lookup"><span data-stu-id="95194-209">Mean Squared Error Definition</span></span>](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)
- [<span data-ttu-id="95194-210">What are Mean Squared Error and Root Mean Squared Error?</span><span class="sxs-lookup"><span data-stu-id="95194-210">What are Mean Squared Error and Root Mean Squared Error?</span></span>](https://www.vernier.com/til/1014/)
