---
title: Métriques ML.NET
description: Découvrir les métriques qui sont utilisées pour évaluer les performances d’un modèle ML.NET
ms.date: 12/17/2019
ms.openlocfilehash: 8e823fd8cc344c1b8e0ecd709b527137368cbfa0
ms.sourcegitcommit: 7588136e355e10cbc2582f389c90c127363c02a5
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 03/15/2020
ms.locfileid: "79399216"
---
# <a name="evaluate-your-mlnet-model-with-metrics"></a><span data-ttu-id="feea3-103">Évaluez votre modèle ML.NET avec des mesures</span><span class="sxs-lookup"><span data-stu-id="feea3-103">Evaluate your ML.NET model with metrics</span></span>

<span data-ttu-id="feea3-104">Comprendre les mesures utilisées pour évaluer un modèle ML.NET.</span><span class="sxs-lookup"><span data-stu-id="feea3-104">Understand the metrics used to evaluate an ML.NET model.</span></span>

<span data-ttu-id="feea3-105">Les mesures d’évaluation sont spécifiques au type de tâche d’apprentissage automatique qu’un modèle effectue.</span><span class="sxs-lookup"><span data-stu-id="feea3-105">Evaluation metrics are specific to the type of machine learning task that a model performs.</span></span>

<span data-ttu-id="feea3-106">Par exemple, pour la tâche de classification, le modèle est évalué en mesurant dans quelle mesure une catégorie prédite correspond à la catégorie réelle.</span><span class="sxs-lookup"><span data-stu-id="feea3-106">For example, for the classification task, the model is evaluated by measuring how well a predicted category matches the actual category.</span></span> <span data-ttu-id="feea3-107">Et pour le regroupement, l’évaluation est basée sur la proximité des éléments groupés les uns aux autres, et combien de séparation il ya entre les clusters.</span><span class="sxs-lookup"><span data-stu-id="feea3-107">And for clustering, evaluation is based on how close clustered items are to each other, and how much separation there is between the clusters.</span></span>

## <a name="evaluation-metrics-for-binary-classification"></a><span data-ttu-id="feea3-108">Mesures d’évaluation pour la classification binaire</span><span class="sxs-lookup"><span data-stu-id="feea3-108">Evaluation metrics for Binary Classification</span></span>

| <span data-ttu-id="feea3-109">Mesures</span><span class="sxs-lookup"><span data-stu-id="feea3-109">Metrics</span></span>   |      <span data-ttu-id="feea3-110">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-110">Description</span></span>      |  <span data-ttu-id="feea3-111">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-111">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="feea3-112">**Précision**</span><span class="sxs-lookup"><span data-stu-id="feea3-112">**Accuracy**</span></span> |  <span data-ttu-id="feea3-113">La [précision](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) est la proportion de prédictions correctes avec un jeu de données de test.</span><span class="sxs-lookup"><span data-stu-id="feea3-113">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) is the proportion of correct predictions with a test data set.</span></span> <span data-ttu-id="feea3-114">Elle représente le rapport entre le nombre de prédictions correctes et le nombre total d’échantillons d’entrée.</span><span class="sxs-lookup"><span data-stu-id="feea3-114">It is the ratio of number of correct predictions to the total number of input samples.</span></span> <span data-ttu-id="feea3-115">Il fonctionne bien s’il y a un nombre similaire d’échantillons appartenant à chaque classe.</span><span class="sxs-lookup"><span data-stu-id="feea3-115">It works well if there are similar number of samples belonging to each class.</span></span>| <span data-ttu-id="feea3-116">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-116">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="feea3-117">Toutefois, la valeur exacte 1,00 indique un problème (en règle générale, une fuite d’étiquette/cible, un surapprentissage ou un test avec des données d’entraînement).</span><span class="sxs-lookup"><span data-stu-id="feea3-117">But exactly 1.00 indicates an issue (commonly: label/target leakage, over-fitting, or testing with training data).</span></span> <span data-ttu-id="feea3-118">Lorsque les données de test sont déséquilibrées (lorsque la plupart des instances appartiennent à l’une des classes), le jeu de données est petit, ou les scores approchent 0,00 ou 1,00, alors la précision ne saisit pas vraiment l’efficacité d’un classificateur et vous devez vérifier des mesures supplémentaires.</span><span class="sxs-lookup"><span data-stu-id="feea3-118">When the test data is unbalanced (where most of the instances belong to one of the classes), the dataset is small, or scores approach 0.00 or 1.00, then accuracy doesn’t really capture the effectiveness of a classifier and you need to check additional metrics.</span></span> |
| <span data-ttu-id="feea3-119">**AUC**</span><span class="sxs-lookup"><span data-stu-id="feea3-119">**AUC**</span></span> |    <span data-ttu-id="feea3-120">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) ou *zone sous la courbe* mesure la zone sous la courbe créée en balayant le taux positif réel par rapport au taux de faux positifs.</span><span class="sxs-lookup"><span data-stu-id="feea3-120">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or *Area under the curve* measures the area under the curve created by sweeping the true positive rate vs. the false positive rate.</span></span>  |   <span data-ttu-id="feea3-121">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-121">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="feea3-122">Il devrait être supérieur à 0,50 pour qu’un modèle soit acceptable.</span><span class="sxs-lookup"><span data-stu-id="feea3-122">It should be greater than 0.50 for a model to be acceptable.</span></span> <span data-ttu-id="feea3-123">Un modèle avec AUC de 0,50 ou moins est sans valeur.</span><span class="sxs-lookup"><span data-stu-id="feea3-123">A model with AUC of 0.50 or less is worthless.</span></span> |
| <span data-ttu-id="feea3-124">**Zone sous une courbe de précision/rappel**</span><span class="sxs-lookup"><span data-stu-id="feea3-124">**AUCPR**</span></span> | <span data-ttu-id="feea3-125">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) ou *zone sous la courbe d’une courbe de précision-rappel*: mesure utile du succès de la prédiction lorsque les classes sont déséquilibrées (ensembles de données fortement biaisés).</span><span class="sxs-lookup"><span data-stu-id="feea3-125">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) or *Area under the curve of a Precision-Recall curve*: Useful measure of success of prediction when the classes are imbalanced (highly skewed datasets).</span></span> |  <span data-ttu-id="feea3-126">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-126">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="feea3-127">Des scores élevés proches de 1,00 montrent que le classifieur retourne des résultats précis (précision élevée) ainsi que la majorité de tous les résultats positifs (rappel élevé).</span><span class="sxs-lookup"><span data-stu-id="feea3-127">High scores close to 1.00 show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</span></span> |
| <span data-ttu-id="feea3-128">**Score F1**</span><span class="sxs-lookup"><span data-stu-id="feea3-128">**F1-score**</span></span> | <span data-ttu-id="feea3-129">[Score F1](https://en.wikipedia.org/wiki/F1_score) également appelé *balanced F-score or F-measure*.</span><span class="sxs-lookup"><span data-stu-id="feea3-129">[F1 score](https://en.wikipedia.org/wiki/F1_score) also known as *balanced F-score or F-measure*.</span></span> <span data-ttu-id="feea3-130">Il s’agit de la moyenne harmonique de la précision et du rappel.</span><span class="sxs-lookup"><span data-stu-id="feea3-130">It's the harmonic mean of the precision and recall.</span></span> <span data-ttu-id="feea3-131">Le score F1 est utile quand vous souhaitez rechercher un équilibre entre la précision et le rappel.</span><span class="sxs-lookup"><span data-stu-id="feea3-131">F1 Score is helpful when you want to seek a balance between Precision and Recall.</span></span>| <span data-ttu-id="feea3-132">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-132">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="feea3-133">Un score F1 atteint sa meilleure valeur à 1,00 et la pire à 0,00.</span><span class="sxs-lookup"><span data-stu-id="feea3-133">An F1 score reaches its best value at 1.00 and worst score at 0.00.</span></span> <span data-ttu-id="feea3-134">Il vous indique le degré de précision de votre classifieur.</span><span class="sxs-lookup"><span data-stu-id="feea3-134">It tells you how precise your classifier is.</span></span> |

<span data-ttu-id="feea3-135">Pour plus d’informations sur les métriques de classification binaire, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="feea3-135">For further details on binary classification metrics read the following articles:</span></span>

- [<span data-ttu-id="feea3-136">Précision, précision, rappel ou F1 ?</span><span class="sxs-lookup"><span data-stu-id="feea3-136">Accuracy, Precision, Recall, or F1?</span></span>](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
- [<span data-ttu-id="feea3-137">Classe BinaryClassificationMetrics</span><span class="sxs-lookup"><span data-stu-id="feea3-137">Binary Classification Metrics class</span></span>](xref:Microsoft.ML.Data.BinaryClassificationMetrics)
- [<span data-ttu-id="feea3-138">The Relationship Between Precision-Recall and ROC Curves</span><span class="sxs-lookup"><span data-stu-id="feea3-138">The Relationship Between Precision-Recall and ROC Curves</span></span>](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)

## <a name="evaluation-metrics-for-multi-class-classification"></a><span data-ttu-id="feea3-139">Mesures d’évaluation pour la classification multiclasse</span><span class="sxs-lookup"><span data-stu-id="feea3-139">Evaluation metrics for Multi-class Classification</span></span>

| <span data-ttu-id="feea3-140">Mesures</span><span class="sxs-lookup"><span data-stu-id="feea3-140">Metrics</span></span>   |      <span data-ttu-id="feea3-141">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-141">Description</span></span>      |  <span data-ttu-id="feea3-142">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-142">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="feea3-143">**Micro-précision**</span><span class="sxs-lookup"><span data-stu-id="feea3-143">**Micro-Accuracy**</span></span> |  <span data-ttu-id="feea3-144">La [précision micro-moyenne](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) agrège les contributions de toutes les classes pour calculer la métrique moyenne.</span><span class="sxs-lookup"><span data-stu-id="feea3-144">[Micro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) aggregates the contributions of all classes to compute the average metric.</span></span> <span data-ttu-id="feea3-145">Il s’agit de la fraction d’instances correctement prédites.</span><span class="sxs-lookup"><span data-stu-id="feea3-145">It is the fraction of instances predicted correctly.</span></span> <span data-ttu-id="feea3-146">La micro-moyenne ne tient pas compte de l’appartenance aux classes.</span><span class="sxs-lookup"><span data-stu-id="feea3-146">The micro-average does not take class membership into account.</span></span> <span data-ttu-id="feea3-147">Fondamentalement, chaque paire exemple-classe contribue de manière égale à la métrique de précision.</span><span class="sxs-lookup"><span data-stu-id="feea3-147">Basically, every sample-class pair contributes equally to the accuracy metric.</span></span> | <span data-ttu-id="feea3-148">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-148">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="feea3-149">Dans une tâche de classification multiclasse, la micro-précision est préférable à la macro-précision si vous suspectez un déséquilibre de classes éventuel (</span><span class="sxs-lookup"><span data-stu-id="feea3-149">In a multi-class classification task, micro-accuracy is preferable over macro-accuracy if you suspect there might be class imbalance (i.e</span></span> <span data-ttu-id="feea3-150">vous avez peut-être beaucoup plus d’exemples d’une classe que d’autres classes).</span><span class="sxs-lookup"><span data-stu-id="feea3-150">you may have many more examples of one class than of other classes).</span></span>|
| <span data-ttu-id="feea3-151">**Macro-précision**</span><span class="sxs-lookup"><span data-stu-id="feea3-151">**Macro-Accuracy**</span></span> | <span data-ttu-id="feea3-152">La [précision macro-moyenne](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) est la précision moyenne au niveau de la classe.</span><span class="sxs-lookup"><span data-stu-id="feea3-152">[Macro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) is the average accuracy at the class level.</span></span> <span data-ttu-id="feea3-153">La précision pour chaque classe est calculée et la macro-précision est la moyenne de ces précisions.</span><span class="sxs-lookup"><span data-stu-id="feea3-153">The accuracy for each class is computed and the macro-accuracy is the average of these accuracies.</span></span> <span data-ttu-id="feea3-154">Fondamentalement, chaque classe contribue de manière égale à la métrique de précision.</span><span class="sxs-lookup"><span data-stu-id="feea3-154">Basically, every class contributes equally to the accuracy metric.</span></span> <span data-ttu-id="feea3-155">Les classes minoritaires sont aussi importantes que les classes plus grandes.</span><span class="sxs-lookup"><span data-stu-id="feea3-155">Minority classes are given equal weight as the larger classes.</span></span> <span data-ttu-id="feea3-156">La métrique de macro-moyenne donne la même pondération à chaque classe, quel que soit le nombre d’instances de cette classe contenues dans le jeu de données.</span><span class="sxs-lookup"><span data-stu-id="feea3-156">The macro-average metric gives the same weight to each class, no matter how many instances from that class the dataset contains.</span></span> |  <span data-ttu-id="feea3-157">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-157">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="feea3-158">La métrique est calculée de manière indépendante pour chaque classe, puis la moyenne est calculée (toutes les classes étant ainsi traitées de façon égale)</span><span class="sxs-lookup"><span data-stu-id="feea3-158">It computes the metric independently for each class and then takes the average (hence treating all classes equally)</span></span> |
| <span data-ttu-id="feea3-159">**Perte de journal**</span><span class="sxs-lookup"><span data-stu-id="feea3-159">**Log-loss**</span></span>| <span data-ttu-id="feea3-160">La [perte logarithmique](http://wiki.fast.ai/index.php/Log_Loss) mesure les performances d’un modèle de classification où l’entrée de prédiction est une valeur de probabilité comprise entre 0,00 et 1,00.</span><span class="sxs-lookup"><span data-stu-id="feea3-160">[Logarithmic loss](http://wiki.fast.ai/index.php/Log_Loss) measures the performance of a classification model where the prediction input is a probability value between 0.00 and 1.00.</span></span> <span data-ttu-id="feea3-161">La perte logarithmique augmente à mesure que la probabilité prédite diffère de l’étiquette réelle.</span><span class="sxs-lookup"><span data-stu-id="feea3-161">Log-loss increases as the predicted probability diverges from the actual label.</span></span> | <span data-ttu-id="feea3-162">**Plus la précision est proche de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-162">**The closer to 0.00, the better**.</span></span> <span data-ttu-id="feea3-163">Un modèle parfait aurait une perte logarithmique de 0,00.</span><span class="sxs-lookup"><span data-stu-id="feea3-163">A perfect model would have a log-loss of 0.00.</span></span> <span data-ttu-id="feea3-164">L’objectif de nos modèles Machine Learning consiste à réduire cette valeur.</span><span class="sxs-lookup"><span data-stu-id="feea3-164">The goal of our machine learning models is to minimize this value.</span></span>|
| <span data-ttu-id="feea3-165">**Réduction de la perte logarithmique**</span><span class="sxs-lookup"><span data-stu-id="feea3-165">**Log-Loss Reduction**</span></span> | <span data-ttu-id="feea3-166">La [réduction de la perte logarithmique](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) peut être interprétée comme exprimant l’avantage du classifieur par rapport à une prédiction aléatoire.</span><span class="sxs-lookup"><span data-stu-id="feea3-166">[Logarithmic loss reduction](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) can be interpreted as the advantage of the classifier over a random prediction.</span></span>| <span data-ttu-id="feea3-167">**Elle est comprise entre -inf et 1,00, où 1,00 correspond à des prédictions parfaites et 0,00 à des prédictions moyennes**.</span><span class="sxs-lookup"><span data-stu-id="feea3-167">**Ranges from -inf and 1.00, where 1.00 is perfect predictions and 0.00 indicates mean predictions**.</span></span> <span data-ttu-id="feea3-168">Par exemple, si la valeur est égale à 0,20, elle peut être interprétée comme « la probabilité d’une prédiction correcte est 20 % meilleure qu’une estimation aléatoire ».</span><span class="sxs-lookup"><span data-stu-id="feea3-168">For example, if the value equals 0.20, it can be interpreted as "the probability of a correct prediction is 20% better than random guessing"</span></span>|

<span data-ttu-id="feea3-169">La micro-précision est généralement mieux alignée sur les besoins métier de prédictions de ML.</span><span class="sxs-lookup"><span data-stu-id="feea3-169">Micro-accuracy is generally better aligned with the business needs of ML predictions.</span></span> <span data-ttu-id="feea3-170">Si vous souhaitez sélectionner une seule métrique pour choisir la qualité d’une tâche de classification multiclasse, ce doit généralement être la micro-précision.</span><span class="sxs-lookup"><span data-stu-id="feea3-170">If you want to select a single metric for choosing the quality of a multiclass classification task, it should usually be micro-accuracy.</span></span>

<span data-ttu-id="feea3-171">Prenons l’exemple d’une tâche de classification de ticket de support (mappage des tickets entrants aux équipes de support technique) :</span><span class="sxs-lookup"><span data-stu-id="feea3-171">Example, for a support ticket classification task: (maps incoming tickets to support teams)</span></span>

- <span data-ttu-id="feea3-172">Micro-précision : avec quelle fréquence un ticket entrant est-il orienté vers l’équipe appropriée ?</span><span class="sxs-lookup"><span data-stu-id="feea3-172">Micro-accuracy -- how often does an incoming ticket get classified to the right team?</span></span>
- <span data-ttu-id="feea3-173">Macro-précision : pour une équipe moyenne, avec quelle fréquence un ticket entrant est-il correct pour l’équipe concernée ?</span><span class="sxs-lookup"><span data-stu-id="feea3-173">Macro-accuracy -- for an average team, how often is an incoming ticket correct for their team?</span></span>

<span data-ttu-id="feea3-174">Macro-précision surpoids petites équipes dans cet exemple; une petite équipe qui obtient seulement 10 billets par an compte autant qu’une grande équipe avec 10k billets par an.</span><span class="sxs-lookup"><span data-stu-id="feea3-174">Macro-accuracy overweights small teams in this example; a small team that gets only 10 tickets per year counts as much as a large team with 10k tickets per year.</span></span> <span data-ttu-id="feea3-175">Dans ce cas, la micro-précision présente une meilleure corrélation avec le besoin métier exprimé par « combien de temps et d’argent l’entreprise peut-elle économiser en automatisant mon processus de routage des tickets ».</span><span class="sxs-lookup"><span data-stu-id="feea3-175">Micro-accuracy in this case correlates better with the business need of, "how much time/money can the company save by automating my ticket routing process".</span></span>

<span data-ttu-id="feea3-176">Pour plus d’informations sur les métriques de classification multiclasse, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="feea3-176">For further details on multi-class classification metrics read the following articles:</span></span>

- [<span data-ttu-id="feea3-177">Micro- et Macro-moyenne de précision, rappel et F-Score</span><span class="sxs-lookup"><span data-stu-id="feea3-177">Micro- and Macro-average of Precision, Recall, and F-Score</span></span>](https://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html)
- [<span data-ttu-id="feea3-178">Multiclass Classification with Imbalanced Dataset</span><span class="sxs-lookup"><span data-stu-id="feea3-178">Multiclass Classification with Imbalanced Dataset</span></span>](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)

## <a name="evaluation-metrics-for-regression-and-recommendation"></a><span data-ttu-id="feea3-179">Mesures d’évaluation pour la régression et la recommandation</span><span class="sxs-lookup"><span data-stu-id="feea3-179">Evaluation metrics for Regression and Recommendation</span></span>

<span data-ttu-id="feea3-180">Les tâches de régression et de recommandation prédisent un nombre.</span><span class="sxs-lookup"><span data-stu-id="feea3-180">Both the regression and recommendation tasks predict a number.</span></span> <span data-ttu-id="feea3-181">Dans le cas de la régression, le nombre peut être n’importe quelle propriété de sortie qui est influencée par les propriétés d’entrée.</span><span class="sxs-lookup"><span data-stu-id="feea3-181">In the case of regression, the number can be any output property that is influenced by the input properties.</span></span> <span data-ttu-id="feea3-182">Pour recommandation, le nombre est généralement une valeur de notation (entre 1 et 5 par exemple), ou une recommandation oui/non (représentée par 1 et 0 respectivement).</span><span class="sxs-lookup"><span data-stu-id="feea3-182">For recommendation, the number is usually a rating value (between 1 and 5 for example), or a yes/no recommendation (represented by 1 and 0 respectively).</span></span>

| <span data-ttu-id="feea3-183">Métrique</span><span class="sxs-lookup"><span data-stu-id="feea3-183">Metric</span></span>   |      <span data-ttu-id="feea3-184">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-184">Description</span></span>      |  <span data-ttu-id="feea3-185">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-185">Look for</span></span> |
|----------|-----------------------|-----------|
| <span data-ttu-id="feea3-186">**R carré**</span><span class="sxs-lookup"><span data-stu-id="feea3-186">**R-Squared**</span></span> |  <span data-ttu-id="feea3-187">Le *coefficient de détermination*, ou [R carré (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), représente la puissance prédictive du modèle sous la forme d’une valeur comprise entre -inf et 1,00.</span><span class="sxs-lookup"><span data-stu-id="feea3-187">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), or *Coefficient of determination* represents the predictive power of the model as a value between -inf and 1.00.</span></span> <span data-ttu-id="feea3-188">1,00 signifie un ajustement parfait ; l’ajustement peut être arbitrairement médiocre, les scores pouvant alors être négatifs.</span><span class="sxs-lookup"><span data-stu-id="feea3-188">1.00 means there is a perfect fit, and the fit can be arbitrarily poor so the scores can be negative.</span></span> <span data-ttu-id="feea3-189">Un score de 0,00 signifie que le modèle devine la valeur attendue pour l’étiquette.</span><span class="sxs-lookup"><span data-stu-id="feea3-189">A score of 0.00 means the model is guessing the expected value for the label.</span></span> <span data-ttu-id="feea3-190">R2 mesure la proximité des valeurs de données de test réelles des valeurs prédites.</span><span class="sxs-lookup"><span data-stu-id="feea3-190">R2 measures how close the actual test data values are to the predicted values.</span></span> | <span data-ttu-id="feea3-191">**Plus la précision est proche de 1,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-191">**The closer to 1.00, the better quality**.</span></span> <span data-ttu-id="feea3-192">Cependant, de faibles valeurs de coefficient de détermination (par exemple 0,50) peuvent parfois être tout à fait normales ou suffisantes pour votre scénario, alors que des valeurs élevées ne conviennent pas toujours et peuvent être suspectes.</span><span class="sxs-lookup"><span data-stu-id="feea3-192">However, sometimes low R-squared values (such as 0.50) can be entirely normal or good enough for your scenario and high R-squared values are not always good and be suspicious.</span></span> |
| <span data-ttu-id="feea3-193">**Perte absolue**</span><span class="sxs-lookup"><span data-stu-id="feea3-193">**Absolute-loss**</span></span> |  <span data-ttu-id="feea3-194">La [perte absolue](https://en.wikipedia.org/wiki/Mean_absolute_error) ou *erreur d’absolue moyenne (MAE)* mesure la proximité des prédictions des résultats réels.</span><span class="sxs-lookup"><span data-stu-id="feea3-194">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) or *Mean absolute error (MAE)* measures how close the predictions are to the actual outcomes.</span></span> <span data-ttu-id="feea3-195">Il s’agit de la moyenne de toutes les erreurs du modèle, où l’erreur de modèle est la distance absolue entre la valeur d’étiquette prédite et la valeur d’étiquette correcte.</span><span class="sxs-lookup"><span data-stu-id="feea3-195">It is the average of all the model errors, where model error is the absolute distance between the predicted label value and the correct label value.</span></span> <span data-ttu-id="feea3-196">Cette erreur de prédiction est calculée pour chaque enregistrement du jeu de données de test.</span><span class="sxs-lookup"><span data-stu-id="feea3-196">This prediction error is calculated for each record of the test data set.</span></span> <span data-ttu-id="feea3-197">Enfin, la valeur moyenne est calculée pour toutes les erreurs d’absolue enregistrées.</span><span class="sxs-lookup"><span data-stu-id="feea3-197">Finally, the mean value is calculated for all recorded absolute errors.</span></span>| <span data-ttu-id="feea3-198">**Plus la précision est proche de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-198">**The closer to 0.00, the better quality.**</span></span> <span data-ttu-id="feea3-199">L’erreur absolue moyenne utilise la même échelle que les données mesurées (n’est pas normalisée à une plage spécifique).</span><span class="sxs-lookup"><span data-stu-id="feea3-199">The mean absolute error uses the same scale as the data being measured (is not normalized to specific range).</span></span> <span data-ttu-id="feea3-200">Vous ne pouvez utiliser l’erreur absolue, l’erreur quadratique moyenne et la racine de l’erreur quadratique moyenne que pour comparer des modèles pour le même jeu de données ou pour un jeu de données présentant une distribution similaire des valeurs d’étiquette.</span><span class="sxs-lookup"><span data-stu-id="feea3-200">Absolute-loss, Squared-loss, and RMS-loss can only be used to make comparisons between models for the same dataset or dataset with a similar label value distribution.</span></span> |
| <span data-ttu-id="feea3-201">**Perte carrée**</span><span class="sxs-lookup"><span data-stu-id="feea3-201">**Squared-loss**</span></span> |  <span data-ttu-id="feea3-202">[Perte carrée](https://en.wikipedia.org/wiki/Mean_squared_error) ou *erreur carrée moyenne (MSE)*, également appelée *Mean Squared Deviation (MSD)*, vous indique à quel point une ligne de régression est proche d’un ensemble de valeurs de données de test en prenant les distances entre les points et la ligne de régression (ces distances sont les erreurs E) et en les quadranant.</span><span class="sxs-lookup"><span data-stu-id="feea3-202">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) or *Mean Squared Error (MSE)*, also called *Mean Squared Deviation (MSD)*, tells you how close a regression line is to a set of test data values by taking the distances from the points to the regression line (these distances are the errors E) and squaring them.</span></span> <span data-ttu-id="feea3-203">L’élévation au carré attribue une pondération supérieure aux différences plus grandes.</span><span class="sxs-lookup"><span data-stu-id="feea3-203">The squaring gives more weight to larger differences.</span></span> | <span data-ttu-id="feea3-204">Elle est toujours non négative, et **plus les valeurs sont proches de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-204">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="feea3-205">En fonction de vos données, il peut s’avérer impossible d’obtenir une valeur très petite pour l’erreur quadratique moyenne.</span><span class="sxs-lookup"><span data-stu-id="feea3-205">Depending on your data, it may be impossible to get a very small value for the mean squared error.</span></span>|
| <span data-ttu-id="feea3-206">**Racine de l’erreur quadratique**</span><span class="sxs-lookup"><span data-stu-id="feea3-206">**RMS-loss**</span></span> |  <span data-ttu-id="feea3-207">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) ou *Root Mean Squared Error (RMSE)* (également appelé *Root Mean Square Deviation, RMSD*), mesure la différence entre les valeurs prédites par un modèle et les valeurs observées à partir de l’environnement qui est modélisé.</span><span class="sxs-lookup"><span data-stu-id="feea3-207">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or *Root Mean Squared Error (RMSE)* (also called *Root Mean Square Deviation, RMSD*), measures the difference between values predicted by a model and the values observed from the environment that is being modeled.</span></span> <span data-ttu-id="feea3-208">La racine de l’erreur quadratique moyenne est la racine carrée de l’erreur quadratique moyenne et a les mêmes unités que l’étiquette, à l’image de l’erreur absolue, bien que les différences plus grandes se voient attribuer une pondération supérieure.</span><span class="sxs-lookup"><span data-stu-id="feea3-208">RMS-loss is the square root of Squared-loss and has the same units as the label, similar to the absolute-loss though giving more weight to larger differences.</span></span> <span data-ttu-id="feea3-209">La racine de l’erreur quadratique moyenne est couramment utilisée dans les domaines de la climatologie, des prévisions et de l’analyse de régression pour vérifier des résultats expérimentaux.</span><span class="sxs-lookup"><span data-stu-id="feea3-209">Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</span></span> | <span data-ttu-id="feea3-210">Elle est toujours non négative, et **plus les valeurs sont proches de 0,00, meilleure est la qualité**.</span><span class="sxs-lookup"><span data-stu-id="feea3-210">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="feea3-211">La racine de l’erreur quadratique moyenne est une mesure de précision, qui compare les erreurs de prévision de différents modèles pour un jeu de données particulier et non entre plusieurs jeux de données, étant dépendante de l’échelle.</span><span class="sxs-lookup"><span data-stu-id="feea3-211">RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.</span></span>|

<span data-ttu-id="feea3-212">Pour plus d’informations sur les métriques de régression, consultez les articles suivants :</span><span class="sxs-lookup"><span data-stu-id="feea3-212">For further details on regression metrics, read the following articles:</span></span>

- [<span data-ttu-id="feea3-213">Analyse de régression : Comment interpréter R-squared et évaluer la bonté de l’ajustement?</span><span class="sxs-lookup"><span data-stu-id="feea3-213">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</span></span>](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
- [<span data-ttu-id="feea3-214">How To Interpret R-squared in Regression Analysis</span><span class="sxs-lookup"><span data-stu-id="feea3-214">How To Interpret R-squared in Regression Analysis</span></span>](https://statisticsbyjim.com/regression/interpret-r-squared-regression)
- [<span data-ttu-id="feea3-215">R-Squared Definition</span><span class="sxs-lookup"><span data-stu-id="feea3-215">R-Squared Definition</span></span>](https://www.investopedia.com/terms/r/r-squared.asp)
- [<span data-ttu-id="feea3-216">Mean Squared Error Definition</span><span class="sxs-lookup"><span data-stu-id="feea3-216">Mean Squared Error Definition</span></span>](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)
- [<span data-ttu-id="feea3-217">What are Mean Squared Error and Root Mean Squared Error?</span><span class="sxs-lookup"><span data-stu-id="feea3-217">What are Mean Squared Error and Root Mean Squared Error?</span></span>](https://www.vernier.com/til/1014/)

## <a name="evaluation-metrics-for-clustering"></a><span data-ttu-id="feea3-218">Mesures d’évaluation pour le clustering</span><span class="sxs-lookup"><span data-stu-id="feea3-218">Evaluation metrics for Clustering</span></span>

| <span data-ttu-id="feea3-219">Métrique</span><span class="sxs-lookup"><span data-stu-id="feea3-219">Metric</span></span>   |      <span data-ttu-id="feea3-220">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-220">Description</span></span>      |  <span data-ttu-id="feea3-221">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-221">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="feea3-222">**Distance moyenne**</span><span class="sxs-lookup"><span data-stu-id="feea3-222">**Average Distance**</span></span>|<span data-ttu-id="feea3-223">Moyenne de la distance entre les points de données et le centre de leur cluster assigné.</span><span class="sxs-lookup"><span data-stu-id="feea3-223">Average of the distance between data points and the center of their assigned cluster.</span></span> <span data-ttu-id="feea3-224">La distance moyenne est une mesure de la proximité des points de données aux centroïdes de cluster.</span><span class="sxs-lookup"><span data-stu-id="feea3-224">The average distance is a measure of proximity of the data points to cluster centroids.</span></span> <span data-ttu-id="feea3-225">C’est une mesure de la façon dont «serré» le cluster est.</span><span class="sxs-lookup"><span data-stu-id="feea3-225">It's a measure of how 'tight' the cluster is.</span></span>|<span data-ttu-id="feea3-226">Les valeurs plus proches de **0** sont meilleures.</span><span class="sxs-lookup"><span data-stu-id="feea3-226">Values closer to **0** are better.</span></span> <span data-ttu-id="feea3-227">Plus la distance moyenne est proche de zéro, plus les données sont regroupées.</span><span class="sxs-lookup"><span data-stu-id="feea3-227">The closer to zero the average distance is, the more clustered the data is.</span></span> <span data-ttu-id="feea3-228">Notez cependant que cette mesure diminuera si le nombre de clusters est augmenté, et dans le cas extrême (où chaque point de données distinct est son propre cluster), il sera égal à zéro.</span><span class="sxs-lookup"><span data-stu-id="feea3-228">Note though, that this metric will decrease if the number of clusters is increased, and in the extreme case (where each distinct data point is its own cluster) it will be equal to zero.</span></span>
|<span data-ttu-id="feea3-229">**Indice Davies Bouldin**</span><span class="sxs-lookup"><span data-stu-id="feea3-229">**Davies Bouldin Index**</span></span>|<span data-ttu-id="feea3-230">Le rapport moyen des distances à l’intérieur du cluster par rapport aux distances entre les grappes.</span><span class="sxs-lookup"><span data-stu-id="feea3-230">The average ratio of within-cluster distances to between-cluster distances.</span></span> <span data-ttu-id="feea3-231">Plus le cluster est serré et plus les clusters sont éloignés, plus cette valeur est faible.</span><span class="sxs-lookup"><span data-stu-id="feea3-231">The tighter the cluster, and the further apart the clusters are, the lower this value is.</span></span>|<span data-ttu-id="feea3-232">Les valeurs plus proches de **0** sont meilleures.</span><span class="sxs-lookup"><span data-stu-id="feea3-232">Values closer to **0** are better.</span></span> <span data-ttu-id="feea3-233">Les grappes qui sont plus éloignées les unes des autres et moins dispersées se traduiront par un meilleur score.</span><span class="sxs-lookup"><span data-stu-id="feea3-233">Clusters that are farther apart and less dispersed will result in a better score.</span></span>|
|<span data-ttu-id="feea3-234">**Informations mutuelles normalisées**</span><span class="sxs-lookup"><span data-stu-id="feea3-234">**Normalized Mutual Information**</span></span>|<span data-ttu-id="feea3-235">Peut être utilisé lorsque les données de formation utilisées pour former le modèle de clustering est également livré avec des étiquettes de vérité au sol (c’est-à-dire, clustering supervisé).</span><span class="sxs-lookup"><span data-stu-id="feea3-235">Can be used when the training data used to train the clustering model also comes with ground truth labels (that is, supervised clustering).</span></span> <span data-ttu-id="feea3-236">La mesure de l’information mutuelle normalisée mesure si des points de données similaires sont attribués au même cluster et des points de données disparates sont attribués à différents groupes.</span><span class="sxs-lookup"><span data-stu-id="feea3-236">The Normalized Mutual Information metric measures whether similar data points get assigned to the same cluster and disparate data points get assigned to different clusters.</span></span> <span data-ttu-id="feea3-237">L’information mutuelle normalisée est une valeur comprise entre 0 et 1</span><span class="sxs-lookup"><span data-stu-id="feea3-237">Normalized mutual information is a value between 0 and 1</span></span>|<span data-ttu-id="feea3-238">Les valeurs plus proches de **1** sont meilleures</span><span class="sxs-lookup"><span data-stu-id="feea3-238">Values closer to **1** are better</span></span>|

## <a name="evaluation-metrics-for-ranking"></a><span data-ttu-id="feea3-239">Mesures d’évaluation pour le classement</span><span class="sxs-lookup"><span data-stu-id="feea3-239">Evaluation metrics for Ranking</span></span>

| <span data-ttu-id="feea3-240">Métrique</span><span class="sxs-lookup"><span data-stu-id="feea3-240">Metric</span></span>   |      <span data-ttu-id="feea3-241">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-241">Description</span></span>      |  <span data-ttu-id="feea3-242">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-242">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="feea3-243">**Gains cumulatifs actualisés**</span><span class="sxs-lookup"><span data-stu-id="feea3-243">**Discounted Cumulative Gains**</span></span>|<span data-ttu-id="feea3-244">Le gain cumulatif réduit (DCG) est une mesure de la qualité de classement.</span><span class="sxs-lookup"><span data-stu-id="feea3-244">Discounted cumulative gain (DCG) is a measure of ranking quality.</span></span> <span data-ttu-id="feea3-245">Il est dérivé de deux hypothèses.</span><span class="sxs-lookup"><span data-stu-id="feea3-245">It is derived from two assumptions.</span></span> <span data-ttu-id="feea3-246">Un: Les éléments très pertinents sont plus utiles lors de l’apparition plus élevée dans l’ordre de classement.</span><span class="sxs-lookup"><span data-stu-id="feea3-246">One: Highly relevant items are more useful when appearing higher in ranking order.</span></span> <span data-ttu-id="feea3-247">Deux : L’utilité suit la pertinence qui est, plus la pertinence est élevée, plus un élément est utile.</span><span class="sxs-lookup"><span data-stu-id="feea3-247">Two: Usefulness tracks relevance that is, the higher the relevance, the more useful an item.</span></span> <span data-ttu-id="feea3-248">Le gain cumulatif actualisé est calculé pour une position particulière dans l’ordre de classement.</span><span class="sxs-lookup"><span data-stu-id="feea3-248">Discounted cumulative gain is calculated for a particular position in the ranking order.</span></span> <span data-ttu-id="feea3-249">Il résume la pertinence du classement divisé par le logarithme de l’indice de classement jusqu’à la position d’intérêt.</span><span class="sxs-lookup"><span data-stu-id="feea3-249">It sums the relevance grading divided by the logarithm of the ranking index up to the position of interest.</span></span> <span data-ttu-id="feea3-250">Il est calculé à sum_ l’aide de $0 <1> 'i'0 'p' 'frac 'rel_i' 'log_ 'e 'i '1 ' $ Classements de pertinence sont fournis à un algorithme de formation de classement comme étiquettes de vérité au sol.</span><span class="sxs-lookup"><span data-stu-id="feea3-250">It is calculated using $\sum_{i=0}^{p} \frac {rel_i} {\log_{e}{i+1}}$ Relevance gradings are provided to a ranking training algorithm as ground truth labels.</span></span> <span data-ttu-id="feea3-251">Une valeur DCG est fournie pour chaque position dans le tableau de classement, d’où le nom Discounted Cumulative **Gains**.</span><span class="sxs-lookup"><span data-stu-id="feea3-251">One DCG value is provided for each position in the ranking table, hence the name Discounted Cumulative **Gains**.</span></span> |<span data-ttu-id="feea3-252">**Les valeurs plus élevées sont meilleures**</span><span class="sxs-lookup"><span data-stu-id="feea3-252">**Higher values are better**</span></span>|
|<span data-ttu-id="feea3-253">**Gains cumulatifs à prix réduit normalisés**</span><span class="sxs-lookup"><span data-stu-id="feea3-253">**Normalized Discounted Cumulative Gains**</span></span>|<span data-ttu-id="feea3-254">La normalisation du DCG permet de comparer la métrique pour les listes de classement de différentes longueurs</span><span class="sxs-lookup"><span data-stu-id="feea3-254">Normalizing DCG allows the metric to be compared for ranking lists of different lengths</span></span>|<span data-ttu-id="feea3-255">**Les valeurs plus proches de 1 sont meilleures**</span><span class="sxs-lookup"><span data-stu-id="feea3-255">**Values closer to 1 are better**</span></span>|

## <a name="evaluation-metrics-for-anomaly-detection"></a><span data-ttu-id="feea3-256">Mesures d’évaluation pour la détection des anomalies</span><span class="sxs-lookup"><span data-stu-id="feea3-256">Evaluation metrics for Anomaly Detection</span></span>

| <span data-ttu-id="feea3-257">Métrique</span><span class="sxs-lookup"><span data-stu-id="feea3-257">Metric</span></span>   |      <span data-ttu-id="feea3-258">Description</span><span class="sxs-lookup"><span data-stu-id="feea3-258">Description</span></span>      |  <span data-ttu-id="feea3-259">Recherche</span><span class="sxs-lookup"><span data-stu-id="feea3-259">Look for</span></span> |
|----------|-----------------------|-----------|
|<span data-ttu-id="feea3-260">**Zone sous la courbe ROC**</span><span class="sxs-lookup"><span data-stu-id="feea3-260">**Area Under ROC Curve**</span></span>|<span data-ttu-id="feea3-261">La zone sous la courbe de l’opérateur du récepteur mesure la façon dont le modèle sépare les points de données anormaux et habituels.</span><span class="sxs-lookup"><span data-stu-id="feea3-261">Area under the receiver operator curve measures how well the model separates anomalous and usual data points.</span></span>|<span data-ttu-id="feea3-262">**Les valeurs plus proches de 1 sont meilleures**.</span><span class="sxs-lookup"><span data-stu-id="feea3-262">**Values closer to 1 are better**.</span></span> <span data-ttu-id="feea3-263">Seules des valeurs supérieures à 0,5 démontrent l’efficacité du modèle.</span><span class="sxs-lookup"><span data-stu-id="feea3-263">Only values greater than 0.5 demonstrate effectiveness of the model.</span></span> <span data-ttu-id="feea3-264">Les valeurs de 0,5 ou moins indiquent que le modèle n’est pas meilleur que d’allouer au hasard les intrants aux catégories anormales et habituelles</span><span class="sxs-lookup"><span data-stu-id="feea3-264">Values of 0.5 or below indicate that the model is no better than randomly allocating the inputs to anomalous and usual categories</span></span>|
|<span data-ttu-id="feea3-265">**Taux de détection au faux nombre positif**</span><span class="sxs-lookup"><span data-stu-id="feea3-265">**Detection Rate At False Positive Count**</span></span>|<span data-ttu-id="feea3-266">Le taux de détection à un faux décompte positif est le rapport entre le nombre d’anomalies correctement identifiées et le nombre total d’anomalies dans un ensemble de tests, indexé par chaque faux positif.</span><span class="sxs-lookup"><span data-stu-id="feea3-266">Detection rate at false positive count is the ratio of the number of correctly identified anomalies to the total number of anomalies in a test set, indexed by each false positive.</span></span> <span data-ttu-id="feea3-267">Autrement dit, il ya une valeur pour le taux de détection à un faux compte positif pour chaque élément faux positif.</span><span class="sxs-lookup"><span data-stu-id="feea3-267">That is, there is a value for detection rate at false positive count for each false positive item.</span></span>|<span data-ttu-id="feea3-268">**Les valeurs plus proches de 1 sont meilleures**.</span><span class="sxs-lookup"><span data-stu-id="feea3-268">**Values closer to 1 are better**.</span></span> <span data-ttu-id="feea3-269">S’il n’y a pas de faux positifs, alors cette valeur est de 1</span><span class="sxs-lookup"><span data-stu-id="feea3-269">If there are no false positives, then this value is 1</span></span>|
