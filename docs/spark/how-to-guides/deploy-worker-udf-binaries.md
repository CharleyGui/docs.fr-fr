---
title: Déployer .NET pour apache Spark travailleur et binaires fonction défini par l’utilisateur
description: Apprenez à déployer .NET pour apache Spark travailleur et binaires fonction défini par l’utilisateur.
ms.date: 01/21/2019
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: f373ccee398149adcadeac91f02d9896214706b0
ms.sourcegitcommit: 7588136e355e10cbc2582f389c90c127363c02a5
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 03/15/2020
ms.locfileid: "79187596"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="ee0b1-103">Déployer .NET pour apache Spark travailleur et binaires fonction défini par l’utilisateur</span><span class="sxs-lookup"><span data-stu-id="ee0b1-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="ee0b1-104">Cette façon de fournir des instructions générales sur la façon de déployer .NET pour apache Spark travailleur et binaires fonction défini par l’utilisateur.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="ee0b1-105">Vous apprenez quelles variables de l’environnement à configurer, ainsi `spark-submit`que certains paramètres couramment utilisés pour lancer des applications avec .</span><span class="sxs-lookup"><span data-stu-id="ee0b1-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

## <a name="configurations"></a><span data-ttu-id="ee0b1-106">Configurations</span><span class="sxs-lookup"><span data-stu-id="ee0b1-106">Configurations</span></span>
<span data-ttu-id="ee0b1-107">Les configurations affichent les variables et paramètres de l’environnement général afin de déployer .NET pour le travailleur Apache Spark et les binaires de fonction définis par l’utilisateur.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="ee0b1-108">Variables d'environnement</span><span class="sxs-lookup"><span data-stu-id="ee0b1-108">Environment variables</span></span>
<span data-ttu-id="ee0b1-109">Lors du déploiement des travailleurs et de l’écriture de FUD, il y a quelques variables d’environnement couramment utilisées que vous devrez peut-être définir :</span><span class="sxs-lookup"><span data-stu-id="ee0b1-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span>

| <span data-ttu-id="ee0b1-110">Variable d’environnement</span><span class="sxs-lookup"><span data-stu-id="ee0b1-110">Environment Variable</span></span>         | <span data-ttu-id="ee0b1-111">Description</span><span class="sxs-lookup"><span data-stu-id="ee0b1-111">Description</span></span>
| :--------------------------- | :----------
| <span data-ttu-id="ee0b1-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="ee0b1-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="ee0b1-113">Chemin où le <code>Microsoft.Spark.Worker</code> binaire a été généré.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="ee0b1-114">Il est utilisé par le pilote Spark et sera transmis aux exécuteurs testamentaires Spark.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="ee0b1-115">Si cette variable n’est pas configuré, les <code>PATH</code> exécuteurs Spark rechercheront le chemin spécifié dans la variable de l’environnement.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="ee0b1-116">_par exemple " C: 'bin’Microsoft.Spark.Worker"_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="ee0b1-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="ee0b1-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="ee0b1-118">Chemins séparés par <code>Microsoft.Spark.Worker</code> la comma où chargeront les assemblages.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="ee0b1-119">Notez que si un chemin commence par ".", le répertoire de travail sera prépendi.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="ee0b1-120">Si en **mode fil,**"." représenterait l’annuaire de travail du conteneur.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="ee0b1-121">_par exemple " C:\\&lt;'Users&gt;\\&lt;nom&gt;d’utilisateur mysparkapp 'bin’Debug\\&lt;dotnet version&gt;"_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="ee0b1-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="ee0b1-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="ee0b1-123">Si vous voulez <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">déboguer un UDF</a>, <code>1</code> puis <code>spark-submit</code>définir cette variable d’environnement avant d’exécuter .</span><span class="sxs-lookup"><span data-stu-id="ee0b1-123">If you want to <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="ee0b1-124">Options de paramètre</span><span class="sxs-lookup"><span data-stu-id="ee0b1-124">Parameter options</span></span>
<span data-ttu-id="ee0b1-125">Une fois que l’application Spark est `spark-submit` [groupée,](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies)vous pouvez la lancer à l’aide de .</span><span class="sxs-lookup"><span data-stu-id="ee0b1-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="ee0b1-126">Le tableau suivant présente quelques-unes des options couramment utilisées :</span><span class="sxs-lookup"><span data-stu-id="ee0b1-126">The following table shows some of the commonly used options:</span></span>

| <span data-ttu-id="ee0b1-127">Nom du paramètre</span><span class="sxs-lookup"><span data-stu-id="ee0b1-127">Parameter Name</span></span>        | <span data-ttu-id="ee0b1-128">Description</span><span class="sxs-lookup"><span data-stu-id="ee0b1-128">Description</span></span>
| :---------------------| :----------
| <span data-ttu-id="ee0b1-129">--classe</span><span class="sxs-lookup"><span data-stu-id="ee0b1-129">--class</span></span>               | <span data-ttu-id="ee0b1-130">Le point d’entrée de votre demande.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-130">The entry point for your application.</span></span></br><span data-ttu-id="ee0b1-131">_par exemple org.apache.spark.deploy.dotnet.DotnetRunner_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="ee0b1-132">--maître</span><span class="sxs-lookup"><span data-stu-id="ee0b1-132">--master</span></span>              | <span data-ttu-id="ee0b1-133"><a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">L’URL principale</a> pour le cluster.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="ee0b1-134">_par exemple, les fils_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-134">_e.g. yarn_</span></span>
| <span data-ttu-id="ee0b1-135">--déploiement-mode</span><span class="sxs-lookup"><span data-stu-id="ee0b1-135">--deploy-mode</span></span>         | <span data-ttu-id="ee0b1-136">Que ce soit pour déployer votre<code>cluster</code>chauffeur sur les nœuds du travailleur ( ) ou localement en tant que client externe (<code>client</code>).</span><span class="sxs-lookup"><span data-stu-id="ee0b1-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="ee0b1-137">Valeur par défaut : <code>client</code></span><span class="sxs-lookup"><span data-stu-id="ee0b1-137">Default: <code>client</code></span></span>
| <span data-ttu-id="ee0b1-138">--conf</span><span class="sxs-lookup"><span data-stu-id="ee0b1-138">--conf</span></span>                | <span data-ttu-id="ee0b1-139">Propriété arbitraire de <code>key=value</code> configuration Spark en format.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="ee0b1-140">_par exemple spark.yarn.appMasterEnv.DOTNET_WORKER_DIR._</span><span class="sxs-lookup"><span data-stu-id="ee0b1-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="ee0b1-141">--fichiers</span><span class="sxs-lookup"><span data-stu-id="ee0b1-141">--files</span></span>               | <span data-ttu-id="ee0b1-142">Liste séparée par les comma des fichiers à placer dans le répertoire de travail de chaque exécuteur testamentaire.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="ee0b1-143">Veuillez noter que cette option ne s’applique qu’au mode fil.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="ee0b1-144">Il prend en charge la spécifier les noms de fichiers avec 'similaire à Hadoop.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="ee0b1-145">_par exemple <code>myLocalSparkApp.dll#appSeen.dll</code>. Votre application doit utiliser <code>appSeen.dll</code> le <code>myLocalSparkApp.dll</code> nom de référence lors de l’exécution sur YARN._</span><span class="sxs-lookup"><span data-stu-id="ee0b1-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="ee0b1-146">--archives</span><span class="sxs-lookup"><span data-stu-id="ee0b1-146">--archives</span></span>          | <span data-ttu-id="ee0b1-147">Liste d’archives séparée par les comma qui sera extraite dans le répertoire de travail de chaque exécuteur testamentaire.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="ee0b1-148">Veuillez noter que cette option ne s’applique qu’au mode fil.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="ee0b1-149">Il prend en charge la spécifier les noms de fichiers avec 'similaire à Hadoop.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="ee0b1-150">_par exemple <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. Cela copiera et extraira <code>worker</code> le fichier zip au dossier._</span><span class="sxs-lookup"><span data-stu-id="ee0b1-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="ee0b1-151">application-jar</span><span class="sxs-lookup"><span data-stu-id="ee0b1-151">application-jar</span></span>       | <span data-ttu-id="ee0b1-152">Chemin vers un pot groupé, y compris votre application et toutes les dépendances.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="ee0b1-153">_par exemple, hdfs://&lt;chemin vers votre&gt;pot /microsoft-spark-version&lt;&gt;.jar_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="ee0b1-154">application-arguments</span><span class="sxs-lookup"><span data-stu-id="ee0b1-154">application-arguments</span></span> | <span data-ttu-id="ee0b1-155">Arguments transmis à la méthode principale de votre classe principale, le cas échéant.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="ee0b1-156">_par exemple, hdfs://&lt;chemin vers&gt;/&lt;votre&gt;application &lt;votre application&gt; &lt;.zip votre app args nom d’application&gt;_</span><span class="sxs-lookup"><span data-stu-id="ee0b1-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="ee0b1-157">Spécifiez tout le précédent `--options` `application-jar` lors du lancement des applications avec `spark-submit`, sinon ils seront ignorés.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="ee0b1-158">Pour plus d’informations, voir [ `spark-submit` les options](https://spark.apache.org/docs/latest/submitting-applications.html) et [l’étincelle en cours d’exécution sur les détails YARN](https://spark.apache.org/docs/latest/running-on-yarn.html).</span><span class="sxs-lookup"><span data-stu-id="ee0b1-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="ee0b1-159">Forum aux questions</span><span class="sxs-lookup"><span data-stu-id="ee0b1-159">Frequently asked questions</span></span>
### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="ee0b1-160">Lorsque j’exécute une application d’étincelle avec des DUDF, je reçois une erreur 'FileNotFoundException'.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="ee0b1-161">Que dois-je faire ?</span><span class="sxs-lookup"><span data-stu-id="ee0b1-161">What should I do?</span></span>
> <span data-ttu-id="ee0b1-162">**Erreur:** [Erreur] [TaskRunner] [0] ProcessStream () a échoué à l’exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version '1.0.0.0, Culture’neutral, PublicKeyToken’null' fichier introuvable: 'mySparkApp.dll'</span><span class="sxs-lookup"><span data-stu-id="ee0b1-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="ee0b1-163">**Réponse:** Vérifiez que `DOTNET_ASSEMBLY_SEARCH_PATHS` la variable d’environnement est réglée correctement.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="ee0b1-164">Il devrait être le `mySparkApp.dll`chemin qui contient votre .</span><span class="sxs-lookup"><span data-stu-id="ee0b1-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="ee0b1-165">Après avoir mis à niveau mon .NET `DOTNET_WORKER_DIR` pour la version Apache Spark `IOException` et réinitialiser la variable de l’environnement, pourquoi dois-je encore obtenir l’erreur suivante?</span><span class="sxs-lookup"><span data-stu-id="ee0b1-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>
> <span data-ttu-id="ee0b1-166">**Erreur:** Perte de la tâche 0.0 dans l’étape 11.0 (TID 24, localhost, pilote d’exécuteur): java.io.IOException: Impossible d’exécuter le programme "Microsoft.Spark.Worker.exe": CreateProcess erreur 2, Le système ne peut pas trouver le fichier spécifié.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="ee0b1-167">**Réponse:** Essayez de redémarrer votre fenêtre PowerShell (ou d’autres fenêtres de commande) d’abord afin qu’il puisse prendre les dernières valeurs variables de l’environnement.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="ee0b1-168">Alors commencez votre programme.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="ee0b1-169">Après avoir soumis ma demande Spark, je reçois l’erreur `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>
> <span data-ttu-id="ee0b1-170">**Erreur:** [Erreur] [TaskRunner] [0] ProcessStream () a échoué à l’exception: System.TypeLoadException: Impossible de charger le type 'System.Runtime.Remoting.Contexts.Context' de l’assemblage 'mscorlib, Version '4.0.0.0, Culture’neutre, PublicKeyToken...'.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="ee0b1-171">**Réponse:** Vérifiez `Microsoft.Spark.Worker` la version que vous utilisez.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="ee0b1-172">Il existe deux versions: **.NET Framework 4.6.1** et **.NET Core 2.1.x**.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 2.1.x**.</span></span> <span data-ttu-id="ee0b1-173">Dans ce `Microsoft.Spark.Worker.net461.win-x64-<version>` cas, (que vous pouvez `System.Runtime.Remoting.Contexts.Context` [télécharger](https://github.com/dotnet/spark/releases)) doit être utilisé depuis est seulement pour .NET Framework.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="ee0b1-174">Comment puis-je exécuter mon application d’étincelles avec des FUD sur YARN ?</span><span class="sxs-lookup"><span data-stu-id="ee0b1-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="ee0b1-175">Quelles variables et paramètres de l’environnement dois-je utiliser ?</span><span class="sxs-lookup"><span data-stu-id="ee0b1-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="ee0b1-176">**Réponse:** Pour lancer l’application d’étincelles sur YARN, les variables de l’environnement doivent être spécifiées comme `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span><span class="sxs-lookup"><span data-stu-id="ee0b1-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="ee0b1-177">S’il vous plaît `spark-submit`voir ci-dessous comme un exemple en utilisant:</span><span class="sxs-lookup"><span data-stu-id="ee0b1-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-2.4.x-<version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="ee0b1-178">Étapes suivantes</span><span class="sxs-lookup"><span data-stu-id="ee0b1-178">Next steps</span></span>

* [<span data-ttu-id="ee0b1-179">Bien démarrer avec .NET pour Apache Spark</span><span class="sxs-lookup"><span data-stu-id="ee0b1-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="ee0b1-180">Déboguer une application .NET pour Apache Spark sur Windows</span><span class="sxs-lookup"><span data-stu-id="ee0b1-180">Debug a .NET for Apache Spark application on Windows</span></span>](../how-to-guides/debug.md)
